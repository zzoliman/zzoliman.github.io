<!-- By Kayo Yin.
Built on https://startbootstrap.com/theme/grayscale.
Copyright Kayo Yin 2021.
All Rights Reserved.
Do not copy or distribute without permission of the author. -->


<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Kayo Yin</title>
        <link rel="icon" type="image/x-icon" href="assets/totoro.gif" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />

        <script language="javascript">
            function show( elem ) {
                var x = document.getElementById(elem);
                if (x.style.display === "block") {x.style.display = "none";}
                else {x.style.display = "block";}
            }
        </script>
        <base target="_blank">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />


    </head>
    <body id="page-top">
        
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="#page-top" target="_self">Kayo Yin</a>
                <button class="navbar-toggler navbar-toggler-right" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto">
                        <li class="nav-item"><a class="nav-link" href="#about" target="_self">About</a></li>
                        <li class="nav-item"><a class="nav-link" href="#research" target="_self">Research</a></li>
                        <li class="nav-item"><a class="nav-link" href="#pubs" target="_self">Publications</a></li>
                        <li class="nav-item"><a class="nav-link" href="#talks" target="_self">Talks</a></li>
                        <li class="nav-item"><a class="nav-link" href="#awards" target="_self">Awards</a></li>
                        <li class="nav-link disabled">|</li>
                        <li class="nav-item"><a class="nav-link" href="media.html" target="_self">Not Research</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
        <div class="container d-flex align-items-center justify-content-center">
          <div class="d-flex justify-content-center">
        <div class="row">
          <div class="smallcol">
            <section class="about-section text-center" id="about">
                    <div class="text-center">
                       
                           <script type="text/javascript">
                                ImageArray = new Array();
                                ImageArray[0] = 'assets/img/flute.png';
                                ImageArray[1] = 'assets/img/erhu.jpg';
                                ImageArray[2] = 'assets/img/piano.jpeg';
                                ImageArray[3] = 'assets/img/saxophone.png';
                            
                            function getRandomImage() {
                                var num = Math.floor( Math.random() * 4);
                                var img = ImageArray[num];
                                return('<img src="' + img + '" width="250px">')
                            
                            }
                            document.write(getRandomImage());
                            </script>
                            <br>
                            <h3 class="text-body mx-auto mt-2 mb-5">Hi, I'm Kayo <br>
                                Bonjour, je suis Kayo<br>
                                ‰ªäÊó•„ÅØ„ÄÅ„Åã„Çà„Å®Áî≥„Åó„Åæ„Åô„ÄÄ<br>
                                „ÄÄ‰Ω†Â•ΩÔºåÊàëÊòØÊÆ∑Á∂∫Â¶§</h3>
                                <tt>[kajo i…¥] </tt><br>
                                <tt>she/her </tt><br>
                                
                                <tt>kayoyü•∏cs.cmu.edu</tt>
                                                 <br>
                                     
                                                 <a href="assets/cv.pdf"><tt>Curriculum Vitae</tt></a> <br>
                                       <a href="https://scholar.google.com/citations?user=Wc8oLVwAAAAJ&hl=en"><img src="assets/img/scholar.png" width="30" height="30"></a>
                                       <a href="https://twitter.com/kayo_yin" title="Twitter"><img src="assets/img/twitter.png" width="30" height="30"></a>
                                       <a href="https://www.linkedin.com/in/kayoyin/" title="Linkedin"><img src="assets/img/linkedin.png" width="30" height="30"></a>
                                       <a href="https://github.com/kayoyin" title="Github"><img src="assets/img/github.png" width="30" height="30"></a>
                                       <a href="https://medium.com/@kayo.yin" title="Medium"><img src="assets/img/medium.png" width="30" height="30"></a> <br> <br>

                                              
                                     </ul>
                            
                            </div></section></div>
                           
                                <div class="bigcol">
                                  <section class="about-section" id="about">
                                    <div class="text-start">

                                    I am a Master's student in Language Technologies at <a href="https://www.lti.cs.cmu.edu"> Carnegie Mellon University</a> working with <a href="http://www.phontron.com/">Prof. Graham Neubig</a> and I am a member of <a href="http://www.cs.cmu.edu/~neulab/"> NeuLab</a>. I received my Bachelor's degree in Mathematics & Computer Science from <a href="https://www.polytechnique.edu/en">√âcole Polytechnique</a> where I defended my thesis on <a href="https://github.com/kayoyin/transformer-slt">Sign Language Translation</a>.
                                    I am fortunate to be supported by the <a href="https://www.siebelscholars.com/">Siebel Scholars Program</a>.
                                <br><br>
                               My goal is to use natural language processing to break down communication barriers between people and give all language users access to technology. 
                               Currently, my <a href="#research" target="_self">research</a> focuses on context-aware neural machine translation, model interpretability, and sign language processing.
                               <br><br>
                               I come from <a href="https://youtu.be/azpuIIz3mCU">Akashi, Japan</a> and grew up speaking 4 languages in <a href="https://youtu.be/_r4sBv4m0aI">Paris, France</a>. Outside research, I enjoy playing <a href="media.html#perf">multiple instruments</a> (my nickname in college was <a href="https://gargantua.polytechnique.fr/siatel-web/app/linkto/mICYYYTGHYK#page=20t">Femme-Orchestre</a>), martial arts, <a href="hiking.html">hiking</a>, being a Buddhist, <a href="https://youtube.com/playlist?list=PLQRDjZrFKXcpL2j5jekIgpju9FPFo08tm">solo traveling</a>, and watching cute dog videos. 
                                 <br><br>
                                 <li>
                                  <b>2021-11-05</b> Gave an invited talk at DeepMind on <a href="assets/slides/deepmind21.pdf">Natural Language Processing for Signed Languages</a> <br>
                                 </li>
                                 <li>
                                  <b>2021-10-07</b> Gave an invited talk at University of Pittsburgh on <a href="assets/slides/upitt21.pdf">Extending Neural Machine Translation to Dialogue and Signed Languages</a>
                                 </li>
                                 <li>
                                   <b>2021-09-23</b> Extremely honored to be selected as a <b><a href="https://www.siebelscholars.com/">Siebel Scholar</a> Class of 2022</b>!
                                 </li>
                                 
                                 <li>
                                  <b>2021-09-17</b> Gave an invited talk at SIGTYP on <a href="https://youtube.com/playlist?list=PLFIGad0NI4ougZCcNIJXUIW0XRnF-FXdJ">Understanding, Improving and Evaluating Context Usage in Context-aware Machine Translation</a>
                                 </li>
                                 <li>
                                  <b>2021-08-26</b> Super happy to have 2 papers accepted to <a href="https://2021.emnlp.org/">EMNLP 2021</a>!
                              </li>
                                 <li>
                                  <b>2021-07-25</b> 1 paper accepted to the <a href="https://sites.google.com/tilburguniversity.edu/at4svl2021/home?authuser=0">AT4SSL workshop</a> at MT Summit 2021!
                                 </li>
                                 <li>
                                  <b>2021-07-05</b> Extremely thrilled to receive the <b>Best Theme Paper</b> award at <a href="https://2021.aclweb.org/">ACL 2021</a>!
                                 </li>
                                 <li>
                                  <b>2021-05-06</b> Super excited to have 3 papers accepted to <a href="https://2021.aclweb.org/">ACL 2021</a>!
                              </li>
                                 <li>
                                  <b>2021-03-01</b> Gave an invited talk at Unbabel on <a href="assets/slides/unbabel21.pdf">Do Context-Aware Translation Models Pay the Right Attention?</a>
                              </li>
                                 <li>
                                  <b>2020-10-18</b> Gave an invited talk at Computer Vision Talks on <a href="https://youtu.be/E5nKeEvoAK0">Sign Language Translation with Transformers</a>
                               </li>
                               <li>
                                <b>2020-09-30</b> My first conference submission was accepted to <a href="https://coling2020.org/"> COLING'2020</a>!
                             </li>
                                 <li>
                                    <b>2020-09-21</b> Extremely honored to be awarded <b>Global Winner in Computer Science</b> at <a href="https://undergraduateawards.com/winners/global-winners-2020">The Global Undergraduate Awards 2020</a>!
                                 </li>
                                 <li>
                                  <b>2020-08-31</b>  Started my Master's degree at CMU LTI!
                               </li>
                               <li>
                                <b>2020-07-25</b>  My undergraduate paper has been accepted to the <a href="https://slrtp.com/">SLRTP workshop</a> at ECCV'20!
                             </li>
                                 </div>
                   
                     
            </section>
  
            <section class="about-section" id="research">
                <div class="text-start ">
                    

                <h6 class="display-6">Research</h6>
                <hr>
                My long-term research goal is to break down communication barriers between people, and between computers and people using Natural Language Processing. I believe that by improving how computers understand and respond to different natural languages, more people can benefit from technology using their preferred language.
                <br>I like to think about how linguistic priors can enhance model performance, and in turn, what computational models and data can teach us about how languages work.
                <br> <br>
                Currently, I am working on challenges in multilingual NLP such as: <br> <br>
                 <li><b>Context-aware Machine Translation</b>: I am interested in when context, either on an intra-sentential (within the current sentence), inter-sentential (across multiple sentences), or extra-linguistic (e.g. social, temporal, cultural) level, is required during translation, and how to model these features in machine translation. 
                  My works examine when translation requires context and how well models perform on these translations (<a href="https://arxiv.org/abs/2109.07446">arxiv'21</a>), how much context-aware models are actually using context (<a href="https://aclanthology.org/2021.acl-long.505/">ACL'21</a>), whether models are using the type of context we expect them to (<a href="https://aclanthology.org/2021.acl-long.65/">ACL'21</a>), and how to encourage models to use more/better context. </li> <br>
                <li><b>Model Interpretability</b>: I am interested in understanding how current NLP models use contextual information to make decisions during language generation, and using this information to guide models to process context more efficiently. I am also interested in how neural networks process language on a fundamental level and how machine intelligence compares with human cognition.</li>
                <br> 
                <li><b>Sign Language Processing</b>: I am interested in modeling signed languages from a linguistic perspective and extending existing language technologies to signed languages.
                  I have argued the importance for the NLP community to include signed languages, both socially and scientifically, and how to get involved (<a href="https://aclanthology.org/2021.acl-long.570/">ACL'21</a>), I researched how to translate a signed language into a spoken language (<a href="https://slrtp.com/papers/extended_abstracts/SLRTP.EA.12.009.paper.pdf">ECCV'20</a>, <a href="https://www.aclweb.org/anthology/2020.coling-main.525/">COLING'20</a>),
               how to perform data augmentation for Sign Language Translation (<a href="https://aclanthology.org/2021.mtsummit-at4ssl.1/">MTSummit21</a>), and how to perform coreference resolution of pronominal pointing signs (<a href="https://aclanthology.org/2021.emnlp-main.405/">EMNLP'21</a>). </li>
                <br> <br>
                Please do not hesitate to reach out if you'd like to chat or collaborate! I am generally responsive to emails and Twitter messages, I am not as responsive as I'd like to be on LinkedIn.
               </div>
            </section> 
            <section class="about-section" id="pubs">
                <div class="text-start">

                <h3 class="display-6">Publications</h3>
                <hr>
                <i>* = equal contribution</i>
                <br><br>

                 <h4>2021</h4>     
                 <ul class="clist">

                  <li> 
                    <a href="https://aclanthology.org/2021.emnlp-main.405/">Signed Coreference Resolution</a>
                    <br>
                    <b>Kayo Yin</b>, Kenneth DeHaan and Malihe Alikhani.
                      <br>
                      <i>Conference on Empirical Methods in Natural Language Processing (EMNLP). November 2021.</i>
                      
                      <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs8')">
                        Abstract
                      </button>
                
                      <a href="https://aclanthology.org/2021.emnlp-main.405.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/kayoyin/scr"> <span class="badge badge-secondary">Code/Data</span></a>
                      <a href="https://youtu.be/jWAukIkICrs"> <span class="badge badge-video">Video</span></a>
                        <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib8')">
                        BibTeX
                      </button>
        
                      <div id="abs8"  class="popup-abs">
                        &nbsp;&nbsp; Coreference resolution is key to many natural language processing tasks and yet has only been explored for spoken languages.
                        In signed languages, space is primarily used to establish reference. Solving coreference resolution for signed languages would not only enable higher-level Sign Language Processing systems, but also enhance our understanding of language in different modalities and of situated references, which are key problems in studying grounded language.
                        In this paper, we: (1) introduce Signed Coreference Resolution, a new challenge for coreference modeling and Sign Language Processing; (2) collect an annotated corpus of German Sign Language with gold labels for coreference together with an annotation software for the task; (3) explore features of hand gesture, iconicity, and spatial situated properties and move forward to propose a set of linguistically informed heuristics and unsupervised models for the task; (4) put forward several proposals about ways to address the complexities of this challenge effectively. Finally, we invite the NLP community to collaborate with signing communities and direct efforts towards SCR to close this gap.
                        </div>
                        
                        <div id="bib8"  class="popup">
                          @inproceedings{yin-etal-2021-signed,<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;title = "Signed Coreference Resolution",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;author = "Yin, Kayo  and
                                DeHaan, Kenneth  and
                                Alikhani, Malihe",<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;month = nov,<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;address = "Online and Punta Cana, Dominican Republic",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.emnlp-main.405",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;pages = "4950--4961",<br>
                          }
                      </div><br><br>

                  <li> 
                    <a href="https://aclanthology.org/2021.emnlp-main.553/">When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection</a>
                    <br>
                    Aditi Chaudhary, <b>Kayo Yin</b>, Antonios Anastasopoulos and Graham Neubig.
                      <br>
                      <i>Conference on Empirical Methods in Natural Language Processing (EMNLP). November 2021.</i>
                      
                      <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs7')">
                        Abstract
                      </button>
                      <a href="https://aclanthology.org/2021.emnlp-main.553.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/Aditi138/LexSelection"> <span class="badge badge-secondary">Code/Data</span></a>
                        <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib7')">
                        BibTeX
                      </button>
        
                      <div id="abs7"  class="popup-abs">
                        &nbsp;&nbsp; Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun ``wall'' has different lexical manifestations in Spanish --  ``pared'' refers to an indoor wall while ``muro'' refers to an outside wall.
                        However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way.
                        In this work, we present a method for automatically identifying fine-grained lexical distinctions, and extracting rules explaining these distinctions in a human- and machine-readable format. We confirm the quality of these extracted rules in a language learning setup for two languages, Spanish and Greek,  where we use the rules to teach non-native speakers when to translate a given ambiguous word into its different possible translations 
                        </div>

                      <div id="bib7"  class="popup">
                         @inproceedings{chaudhary21emnlp,<br>
                          &nbsp;&nbsp;&nbsp;&nbsp; title = "When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection",<br>
                          &nbsp;&nbsp;&nbsp;&nbsp;author = "Chaudhary, Aditi  and
                            Yin, Kayo  and
                            Anastasopoulos, Antonios and
                            Neubig, Graham",<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)",<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;month = nov,<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                      }
                      </div><br><br>


                      <li> 
                        <a href="https://arxiv.org/abs/2109.07446">When Does Translation Require Context? A Data-driven, Multilingual Exploration</a>
                        <br>
                        <b>Kayo Yin*</b>, Patrick Fernandes*, Andr√© F. T. Martins and Graham Neubig.
                          <br>
                          <i>In submission.</i>
                          
                          <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs9')">
                            Abstract
                          </button>
                          <a href="https://arxiv.org/pdf/2109.07446.pdf"><span class="badge badge-pdf"> PDF </span></a>
                          <a href="https://github.com/neulab/contextual-mt"><span class="badge badge-secondary">  Code/Data </span></a>
                            <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib9')">
                            BibTeX
                          </button>
            
                          <div id="abs9"  class="popup-abs">
                            &nbsp;&nbsp; Although proper handling of discourse phenomena significantly contributes to the quality
                            of machine translation (MT), common translation quality metrics do not adequately capture
                            them. Recent works in context-aware MT attempt to target a small set of these phenomena during evaluation. In this paper, we propose a new metric, P-CXMI, which allows us
                            to identify translations that require context systematically and confirm the difficulty of previously studied phenomena as well as uncover
                            new ones that have not been addressed in previous work. We then develop the Multilingual
                            Discourse-Aware (MuDA) benchmark, a series of taggers for these phenomena in 14 different language pairs, which we use to evaluate
                            context-aware MT. We find that state-of-theart context-aware MT models find marginal
                            improvements over context-agnostic models
                            on our benchmark, which suggests current
                            models do not handle these ambiguities effectively. We release code and data to invite
                            the MT research community to increase efforts
                            on context-aware translation on discourse phenomena and languages that are currently overlooked.
                            </div>
    
                          <div id="bib9"  class="popup">
                             @article{yin2021muda,<br>
                              &nbsp;&nbsp;&nbsp;&nbsp; title = "When Does Translation Require Context? A Data-driven, Multilingual Exploration",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;author = "Yin, Kayo  and
                                Fernandes, Patrick and
                                Martins, Andr{\'e} F. T.  and
                                Neubig, Graham",<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "arXiv preprint arXiv:2109.07446",<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;month = sep,<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                          }
                          </div><br><br>

                      <li> 
                        <span class="badge badge-sp"> üèÜ Best Theme Paper </span> <a href="https://aclanthology.org/2021.acl-long.570/">Including Signed Languages in Natural Language Processing</a> 
                      <br>
                      <b>Kayo Yin</b>, Amit Moryossef, Julie Hochgesang, Yoav Goldberg and Malihe Alikhani.
                        <br>
                        <i>Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP). August 2021. </i>
                        
                        <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs2')">
                          Abstract
                        </button>
                        <a href="https://aclanthology.org/2021.acl-long.570.pdf"><span class="badge badge-pdf"> PDF </span></a>
                        <a href="https://youtu.be/AYEIcOsUyWs"> <span class="badge badge-video">Video</span></a>
                          <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib2')">
                          BibTeX
                        </button>
          
                        <div id="abs2"  class="popup-abs">
                          &nbsp;&nbsp; Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.
                          </div>
                        
                        <div id="bib2"  class="popup">
                           @inproceedings{yin-etal-2021-including,<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;title = "Including Signed Languages in Natural Language Processing",<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;author = "Yin, Kayo  and
                              Moryossef, Amit  and
                              Hochgesang, Julie  and
                              Goldberg, Yoav  and
                              Alikhani, Malihe",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;address = "Online",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.acl-long.570",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;pages = "7347--7360",<br>
                              &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.",<br>
                        }
                        </div><br><br>

                 <li> 
                 <a href="https://aclanthology.org/2021.acl-long.65/"> Do Context-Aware Translation Models Pay the Right Attention? </a>
                <br>
                <b>Kayo Yin</b>, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, Andr√© F. T. Martins and Graham Neubig.
                 <br>
                  <i>Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP). August 2021. </i>
                  <br>
                  <button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs1')">
                Abstract
              </button>
              <a href="https://aclanthology.org/2021.acl-long.65.pdf"><span class="badge badge-pdf"> PDF</span></a>
                  <a href="https://github.com/neulab/contextual-mt"><span class="badge badge-secondary">  Code/Data </span></a>
                  <a href="https://youtu.be/IRiy_xpWC-0"> <span class="badge badge-video">Video</span></a>
                <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib1')">
                BibTeX
              </button>

              <div id="abs1"  class="popup-abs">
                &nbsp;&nbsp; Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model's attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.
               </div>
              
              <div id="bib1"  class="popup">
                 @inproceedings{yin-etal-2021-context,<br>
                  &nbsp;&nbsp;&nbsp;&nbsp; title = "Do Context-Aware Translation Models Pay the Right Attention?",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp; author = "Yin, Kayo  and
                    Fernandes, Patrick  and
                    Pruthi, Danish  and
                    Chaudhary, Aditi  and
                    Martins, Andr{\'e} F. T.  and
                    Neubig, Graham",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp; booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;address = "Online",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.acl-long.65",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;pages = "788--801",<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model{'}s attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.",<br>
              }
              </div><br><br>





                 <li> 
                 <a href="https://aclanthology.org/2021.acl-long.505/"> Measuring and Increasing Context Usage in Context-Aware Machine Translation </a>
                <br>
                  Patrick Fernandes, <b>Kayo Yin</b>, Graham Neubig and Andr√© F. T. Martins.
                 <br>
                  <i>Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP). August 2021. </i>
                  
                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs3')">
                    Abstract
                  </button>
                  <a href="https://aclanthology.org/2021.acl-long.505.pdf"><span class="badge badge-pdf"> PDF</span></a>
                      <a href="https://github.com/neulab/contextual-mt"><span class="badge badge-secondary">Code/Data</span></a>
                    <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib3')">
                    BibTeX
                  </button>
    
                  <div id="abs3"  class="popup-abs">
                    &nbsp;&nbsp; Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context -- context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify the usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that conditioning on a longer context has a diminishing effect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method increases context usage and that this reflects on the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.
                    </div>
                  
                  <div id="bib3"  class="popup">
                     @inproceedings{fernandes-etal-2021-measuring,<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;title = "Measuring and Increasing Context Usage in Context-Aware Machine Translation",<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;author = "Fernandes, Patrick  and
                        Yin, Kayo  and
                        Neubig, Graham  and
                        Martins, Andr{\'e} F. T.",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;address = "Online",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Computational Linguistics",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.acl-long.505",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;pages = "6467--6478",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.",<br>
                  }
                  </div><br><br>


               <li> 
                <a href="https://aclanthology.org/2021.mtsummit-at4ssl.1/"> Data Augmentation for Sign Language Gloss Translation</a>
                <br>
                Amit Moryossef*, <b>Kayo Yin*</b>,  Graham Neubig and Yoav Goldberg.
                  <br>
                  <i>18th Biennial Machine Translation Summit (MTSummit) 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL). August 2021.</i>
                  
                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs4')">
                    Abstract
                  </button>
                  <a href="https://aclanthology.org/2021.mtsummit-at4ssl.1.pdf"><span class="badge badge-pdf"> PDF </span></a>
                    <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib4')">
                    BibTeX
                  </button>
    
                  <div id="abs4"  class="popup-abs">
                    &nbsp;&nbsp; Sign language translation (SLT) is often decomposed into video-to-gloss recognition and gloss-to-text translation, where a gloss is a sequence of transcribed spoken-language words in the order in which they are signed. We focus here on gloss-to-text translation, which we treat as a low-resource neural machine translation (NMT) problem. However, unlike traditional low-resource NMT, gloss-to-text translation differs because gloss-text pairs often have a higher lexical overlap and lower syntactic overlap than pairs of spoken languages. We exploit this lexical overlap and handle syntactic divergence by proposing two rule-based heuristics that generate pseudo-parallel gloss-text pairs from monolingual spoken language text. By pre-training on the thus obtained synthetic data, we improve translation from American Sign Language (ASL) to English and German Sign Language (DGS) to German by up to 3.14 and 2.20 BLEU, respectively.
                    </div>
                  
                  <div id="bib4"  class="popup">
                     @inproceedings{moryossef-etal-2021-data,<br>
                      &nbsp;&nbsp;&nbsp;&nbsp; title = "Data Augmentation for Sign Language Gloss Translation",<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;author = "Moryossef, Amit  and
                        Yin, Kayo  and
                        Neubig, Graham  and
                        Goldberg, Yoav",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL)",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;month = aug,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year = "2021",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;address = "Virtual",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;publisher = "Association for Machine Translation in the Americas",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;url = "https://aclanthology.org/2021.mtsummit-at4ssl.1",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;pages = "1--11",<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;abstract = "Sign language translation (SLT) is often decomposed into video-to-gloss recognition and gloss to-text translation, where a gloss is a sequence of transcribed spoken-language words in the order in which they are signed. We focus here on gloss-to-text translation, which we treat as a low-resource neural machine translation (NMT) problem. However, unlike traditional low resource NMT, gloss-to-text translation differs because gloss-text pairs often have a higher lexical overlap and lower syntactic overlap than pairs of spoken languages. We exploit this lexical overlap and handle syntactic divergence by proposing two rule-based heuristics that generate pseudo-parallel gloss-text pairs from monolingual spoken language text. By pre-training on this synthetic data, we improve translation from American Sign Language (ASL) to English and German Sign Language (DGS) to German by up to 3.14 and 2.20 BLEU, respectively.",
                  }
                  </div><br><br> </ul>

      
                <h4>2020</h4>
                          <ul class="clist">
                 <li> 
                  <a href="https://www.aclweb.org/anthology/2020.coling-main.525/">Better Sign Language Translation with STMC-Transformer</a>
                <br>
                <b>Kayo Yin</b> and Jesse Read.
                  <br>
                  <i>Proceedings of the 28th International Conference on Computational Linguistics (COLING 2020).</i> 

                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs5')">
                    Abstract
                  </button>
                  <a href="https://www.aclweb.org/anthology/2020.coling-main.525.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/kayoyin/transformer-slt"> <span class="badge badge-secondary">Code/Data</span></a>
                    <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib5')">
                    BibTeX
                  </button>
    
                  <div id="abs5"  class="popup-abs">
                    &nbsp;&nbsp; Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR) system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. This paper focuses on the translation system and introduces the STMC-Transformer which improves on the current state-of-the-art by over 5 and 7 BLEU respectively on gloss-to-text and video-to-text translation of the PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase of over 16 BLEU. We also demonstrate the problem in current methods that rely on gloss supervision. The video-to-text translation of our STMC-Transformer outperforms translation of GT glosses. This contradicts previous claims that GT gloss translation acts as an upper bound for SLT performance and reveals that glosses are an inefficient representation of sign language. For future SLT research, we therefore suggest an end-to-end training of the recognition and translation models, or using a different sign language annotation scheme.
                </div>
                  
                  <div id="bib5"  class="popup">
                     @inproceedings{yin-read-2020-better, <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;title = "Better Sign Language Translation with {STMC}-Transformer", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;author = "Yin, Kayo  and Read, Jesse", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 28th International Conference on Computational Linguistics", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;month = December, <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year = "2020", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;address = "Barcelona, Spain (Online)",
                        &nbsp;&nbsp;&nbsp;&nbsp;publisher = "International Committee on Computational Linguistics", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;url = "https://www.aclweb.org/anthology/2020.coling-main.525", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;doi = "10.18653/v1/2020.coling-main.525", <br>
                        &nbsp;&nbsp;&nbsp;&nbsp;pages = "5975--5989", <br>
                    }
                  </div> <br><br>

                 <li> <a href="https://slrtp.com/papers/extended_abstracts/SLRTP.EA.12.009.paper.pdf">
                  Attention is All You Sign: Sign Language Translation with Transformers
                  </a>
                <br>
                <b>Kayo Yin</b> and Jesse Read.
                 <br>
                  <i>European Conference on Computer Vision (ECCV) Workshop on Sign Language Recognition, Translation and Production (SLRTP).</i> 
                  
                  <br><button type="button" class="badge badge-abs" href="javascript:void(0)" onclick="show('abs6')">
                    Abstract
                  </button>
                  <a href="https://slrtp.com/papers/extended_abstracts/SLRTP.EA.12.009.paper.pdf"><span class="badge badge-pdf"> PDF </span></a>
                      <a href="https://github.com/kayoyin/transformer-slt"> <span class="badge badge-secondary">Code/Data</span></a>
                      <a href="https://youtu.be/c-87jFd2lQs"> <span class="badge badge-video">Video</span></a>

                   
                      <button type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bib6')">
                    BibTeX
                  </button>
    
                  <div id="abs6"  class="popup-abs">
                    &nbsp;&nbsp; This paper improves the translation system in Sign Language
                    Translation (SLT) by using Transformers. We report a wide range of experimental results for various Transformer setups and introduce a novel
                    end-to-end SLT system combining Spatial-Temporal Multi-Cue (STMC)
                    and Transformer networks. Our methodology improves on the current
                    state-of-the-art by over 5 and 7 BLEU respectively on ground truth
                    (GT) glosses and predicted glosses of the PHOENIX-Weather 2014T
                    dataset. On the ASLG-PC12 corpus, we report an improvement of over
                    16 BLEU. Our findings also reveal that end-to-end translation with predicted glosses outperforms translation on GT glosses. This shows the
                    potential for further improvement in SLT by either jointly training the
                    SLR and translation systems or by revising the gloss annotation scheme. </div>
                  
                  <div id="bib6"  class="popup">
                     @inproceedings{yin2020attention,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;title={{Attention is All You Sign: Sign Language Translation with Transformers}},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;author={Yin, Kayo and Read, Jesse},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;booktitle={Sign Language Recognition, Translation and Production (SLRTP) Workshop-Extended Abstracts},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;volume={4},<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;year={2020}<br>
                      }
                    </ul>
                    </section>
        
              <section id="talks" class="mt-5">
                <div class="text-start">
                <h3 class="display-6">Talks</h3>
                <hr>

                <ul class="clist">

                  <li>  <span class="badge badge-abs">2021</span> <span class="badge badge-video">Talk</span> 
                      <a href="assets/slides/deepmind21.pdf">Natural Language Processing for Signed Languages</a> (<a href="https://deepmind.com/">DeepMind</a>)
                       </li> 
                      </ul>   

                <ul class="clist">

                  <li>  <span class="badge badge-abs">2021</span> <span class="badge badge-secondary">Interview</span> 
                      <a href="https://www.jeanninemanuelalumni.org/news/2021/10/26/kayo-yin-class-of-2017"> Alumni Stories: Kayo Yin, Class of 2017</a>  
                       </li> 
                      </ul>

                      <ul class="clist">

                        <li>  <span class="badge badge-abs">2021</span> <span class="badge badge-video">Talk</span> 
                            <a href="assets/slides/upitt21.pdf">Extending Neural Machine Translation to Dialogue and Signed Languages</a> (<a href="https://www.pitt.edu/">University of Pittsburgh</a>)
                             </li> 
                            </ul>   

                <ul class="clist">
                  <li> <span class="badge badge-abs">2021</span> <span class="badge badge-video">Talk</span> 
                    <a href="https://youtu.be/dgCjT0M7Osc">Understanding, Improving and Evaluating Context Usage in Context-aware Machine Translation</a> (<a href="https://sigtyp.github.io/">SIGTYP</a>)
       
                  
                  <div class="totoro"> <br><br><iframe width="370" height="210" src="https://www.youtube.com/embed/videoseries?list=PLFIGad0NI4ougZCcNIJXUIW0XRnF-FXdJ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> </div> </ul>
      </li> 
      </ul>

                <ul class="clist">

                  <li>  <span class="badge badge-abs">2021</span> <span class="badge badge-secondary">Interview</span> 
                      <a href="https://www.cs.cmu.edu/news/2021/nlp-signed-languages"> LTI Master's Student Urges NLP Focus on Signed Languages</a>  
                       </li> 
                      </ul>

                      <ul class="clist">

                        <li>  <span class="badge badge-abs">2021</span> <span class="badge badge-video">Talk</span> 
                            <a href="assets/slides/unbabel21.pdf">Do Context-Aware Translation Models Pay the Right Attention?</a> (<a href="https://unbabel.com/">Unbabel</a>)
                             </li> 
                            </ul>     

          <ul class="clist">
            <li> <span class="badge badge-abs">2020</span> <span class="badge badge-video">Talk</span> 
              <a href="https://youtu.be/MPit0Oh4reM">Sign Language Translation with Transformers</a> (<a href="https://undergraduateawards.com/">UA Global Summit</a>)
 
            
 <div class="totoro"><br><br> <iframe width="370" height="210" src="https://www.youtube.com/embed/MPit0Oh4reM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> </div>           </ul>
</li> 
</ul>
<ul class="clist">

    <li>  <span class="badge badge-abs">2020</span> <span class="badge badge-secondary">Interview</span> 
        <a href="https://programmes.polytechnique.edu/en/post/bachelor-of-science-alumni-kayo-at-carnegie-mellon-university"> Bachelor of Science Alumni - Kayo at Carnegie Mellon University</a>  
         </li> 
        </ul>
        <ul class="clist">
            <li> <span class="badge badge-abs">2020</span> <span class="badge badge-video">Talk</span> 
              <a href="https://youtu.be/E5nKeEvoAK0">Sign Language Translation with Transformers</a> (<a href="https://www.youtube.com/channel/UCseJlTlqQ2jfW66r-fOYaNg/videos">Computer Vision Talks</a>)
 
            
 <div class="totoro"> <br><br><iframe width="370" height="210" src="https://www.youtube.com/embed/E5nKeEvoAK0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> </div>           </ul>
</li> 
        <ul class="clist">
            <li>  <span class="badge badge-abs">2020</span> <span class="badge badge-secondary">Interview</span> 
                <a href="https://www.polytechnique.edu/en/content/graduate-lx-bachelor-science-kayo-yin-wins-2020-global-undergraduate-awards"> Graduate of l‚ÄôX Bachelor of Science, Kayo Yin wins the 2020 Global Undergraduate Awards</a>  
         </li> 
               </ul>
        <ul class="clist">
            <li>  <span class="badge badge-abs">2020</span> <span class="badge badge-secondary">Interview</span> 
                <a href="https://programmes.polytechnique.edu/en/post/testimony-yin-kayo-bx2020-mathscomputer-science-student"> Testimony: Kayo Yin, BX2020 Maths/Computer Science Student</a>  
         </li> 
        </ul>
        <ul class="clist">
            <li>  <span class="badge badge-abs">2019</span> <span class="badge badge-secondary">Interview</span> 
                <a href="https://programmes.polytechnique.edu/en/post/women-in-science-meet-kayo-yin-bachelor-student">Women in Science - Meet a student from the Bachelor Program </a>  
         </li> 
        </ul>
         
         <ul class="clist">
            <li>  <span class="badge badge-abs">2018</span> <span class="badge badge-secondary">Interview</span> 
              <a href="https://youtu.be/RoZAu4pHCxg">Meet our students - Kayo Yin</a> </li> 
           
         <div class="totoro"><br><br> <iframe width="370" height="210" src="https://www.youtube.com/embed/RoZAu4pHCxg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> </div>
          </ul>
          
          <ul class="clist">
          <li>  <span class="badge badge-abs">2017</span> <span class="badge badge-secondary">Interview</span> 
         <a href="https://gargantua.polytechnique.fr/siatel-web/linkto/mICYYYTGHYK#page=20t">A Lifetime of Science - Kayo Yin, Femme-Orchestre </a>  
         </li>
         </ul>

        </div>

            </section>
            <section id="awards" class="mt-5">
              <div class="text-start">
              <h3 class="display-6">Awards</h3>
              <hr>

              <ul class="clist">
                <li> 
                  <span class="badge badge-abs">2021-2022</span> <a href="https://www.siebelscholars.com/scholar-profile/1994/">Siebel Scholarship</a> (35,000 USD)
                   </li> 
                   <li> 
                    <span class="badge badge-abs">2021</span> <a href="https://2021.aclweb.org/program/accept/#best-theme-paper">ACL Best Theme Paper</a> 
                  </li> 
                  <li> 
                    <span class="badge badge-abs">2020</span> <a href="https://undergraduateawards.com/winners/global-winners-2020">The Global Undergraduate Awards Global Winner</a> 
                  </li> 
                  <li> 
                    <span class="badge badge-abs">2020-2021</span> UC Berkeley Fung Excellence Scholarship (declined)
                  </li> 
                  <li> 
                    <span class="badge badge-abs">2015</span> Jeannine Manuel Award (Top 10%)
                  </li> 
                  <li> 
                    <span class="badge badge-abs">2015</span> <a href="http://www.mathkang.org/finale/finale_2015.php?classe=2">Concours Kangourou des Math√©matiques Finalist</a> (6th place out of 13011)
                  </li> 
                  <li> 
                    <span class="badge badge-abs">2014</span> Jeannine Manuel Award (Top 10%)
                  </li> 
                  <li> 
                    <span class="badge badge-abs">2012</span> <a href="http://www.mathkang.org/finale/finale_2012.php?classe=5">Concours Kangourou des Math√©matiques Finalist</a> (8th place out of 53937)
                  </li> 
                    </ul>   
                  </div>
          
                </section>
          </div></div>      </div>
                        
        </div>
              
            <br>
            <p style="color:#808080; font-size:16px;" align="center" >Copyright ¬© Kayo Yin 2021
              <br> Last updated December 12 2021</p>

              <audio id="audio" src="assets/harp.mp3"></audio>
              <script>
                Array.prototype.random = function () {
  return this[Math.floor((Math.random()*this.length))];
}
              let audio = document.getElementById("audio");
              audio.volume = 0.3;
                var start = [3, 775, 1008, 1240, 1917].random()
                audio.currentTime = start;
                function play() {
                //   var starttimes = [
                //   3, 210, 488, 760, 998, 1229, 1486, 1756, 1910, 2180
                // ];
                //   var start = _.sample(starttimes);
                  return audio.paused ? audio.play() : audio.pause();
                }
              </script>

<div class="totoro">
            <div class="imglink">
              <div style="position:fixed; bottom:20px; right:10px;"><img src="assets/totorobottle.gif" height="100%" onclick="play()" title="‚ô™"></div>
            </div></div>
              <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>